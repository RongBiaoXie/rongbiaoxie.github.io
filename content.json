{"pages":[{"title":"About Me","text":"Education2015-2019: Bachelor degree from Xiamen University. PublicationsCurrently under study. Awards2019.06: Xiamen University Excellent Graduation Thesis, “Detection text based on YOLOv3”.2018.04: 2018 RoboCup Robot World Cup China 2D Simulation Team National Third Prize.2018.04: 2018 American College Students Mathematical Modeling Competition H Award.2018.01: First Prize of the 5th Intelligent Manufacturing Challenge of Xiamen University.2017.12: 2017 National College Student Mathematical Modeling Competition, Fujian Province, Provincial First Prize. Intern Experience2018.06-2018.07: Beijing Qian Xuesen Space Technology Laboratory Production Internship.2018.07-2018.09: Huawei Technologies Co, Brave Star Internship Program. Professional AbilityUnderstand the algorithms and theories of deep learning neural networks, the experience of Python programming and tensorflow deep learning framework.","link":"/about/index.html"},{"title":"About Me","text":"Education2015-2019: Bachelor degree from Xiamen University. PublicationsCurrently under study. Awards2019.06: Xiamen University Excellent Graduation Thesis, “Detection text based on YOLOv3”.2018.04: 2018 RoboCup Robot World Cup China 2D Simulation Team National Third Prize.2018.04: 2018 American College Students Mathematical Modeling Competition H Award.2018.01: First Prize of the 5th Intelligent Manufacturing Challenge of Xiamen University.2017.12: 2017 National College Student Mathematical Modeling Competition, Fujian Province, Provincial First Prize. Intern Experience2018.06-2018.07: Beijing Qian Xuesen Space Technology Laboratory Production Internship.2018.07-2018.09: Huawei Technologies Co, Brave Star Internship Program. Professional AbilityUnderstand the algorithms and theories of deep learning neural networks, the experience of Python programming and tensorflow deep learning framework.","link":"/tags/index.html"}],"posts":[{"title":"Code Analysis Of SMO Algorithm And The Summary Of SVM","text":"&ensp;&ensp;Recently, I have read relevant theoretical knowledge of support vector machine(SVM) and the code implementation of SMO. The main idea is to find a dividing hyperplane in a given data set D.&ensp;&ensp;From Finding the optimal conditions for the maximum interval, converting constraint equations by using Lagrangian dual method and KKT conditions, using SMO algorithm to solve the optimal solution. Last, by using kernel functions to Map to high-dimensional spaces and solve nonlinear classification problems. 一、SVM总结1、SVM模型分类&ensp;&ensp;1）线性可分支持向量机——硬间隔支持向量机（硬间隔最大化）——训练数据线性可分&ensp;&ensp;2）线性支持向量机——软间隔支持向量机（软间隔最大化）——训练数据近似线性可分&ensp;&ensp;3）非线性支持向量机——核技巧及软间隔最大化——训练数据线性不可分 2、间隔和支持向量&ensp;&ensp;以二分类（两属性）为例，如图，样本分布在样本空间上，需要寻找一个超平面将不同样本分开，从图中可知正中间鲁棒性最好。如何确定其方程？ &ensp;&ensp;令超平面方程为 $w^Tx+b=0$ &ensp;&ensp;样本空间中任意点x到超平面距离为, $ \\vec w $ 代表 $w$ 的方向 $|w^Tx+b| \\over ||w||$ = $ \\vec w * x + \\vec b * $ $||b|| \\over ||w||$ &ensp;&ensp;令类别标记为{1,-1}，若超平面能正确分类，则有 $w^Tx_i+b\\geq+1, y_i=+1$ $w^Tx_i+b\\leq-1, y_i=-1$ &ensp;&ensp;这里之所以可以与$\\pm 1$比较，是因为间隔只与wx+b的方向有关，而与大小无关，若存在其他w和b超平面成立，总能通过缩放变换使得上式成立，使得等号成立的数据点称为“支持向量”，并且上式两支持超平面距离，即“间隔”为 $2 \\over ||w||$ &ensp;&ensp;目标是间隔最大化，列出优化方程及约束为 $$max {2 \\over ||w||} 等价于 min {1 \\over 2} ||w||^2$$ $$s.t. y_i(w^Tx+b) \\geq 1$$ &ensp;&ensp;是可以直接利用现成的凸二次规划包计算的，但我们通过使用对偶方法和SMO序列最优算法求解。 3、对偶问题转换优化方程&ensp;&ensp;使用拉格朗日乘子法可得到其对偶问题为$$L(w,b,a) = {1 \\over 2} ||w||^2 + \\displaystyle \\sum^m_{i=1} a_i(1-y_i(w^Tx+b))$$&ensp;&ensp;KKT条件为 $a_i \\geq 0$ $y_i(w^Tx+b)-1 \\geq 0$ $a_i(y_i(w^Tx+b)-1) = 0$ &ensp;&ensp;以$a_i \\geq 0$为前提，当$y_i(w^Tx+b) \\geq 1$时，$L(w,b,a)$的最优值为${1 \\over 2} ||w||^2$，当$y_i(w^Tx+b) < 1$时，$L(w,b,a)$的最优值为$\\infty$. &ensp;&ensp;所以优化${1 \\over 2} ||w||^2$等价于优化$L(w,b,a)(a_i \\geq 0)$，于是我们优化的目标函数为$$min_{w,b}{1 \\over 2}||w||^2 = min_{w,b}max_{a}L(w,b,a)$$&ensp;&ensp;根据西瓜书p406定理，在凸函数满足KKT条件时，主问题的下界等于对偶问题的上界，即$$min_{w,b}max_{a}L(w,b,a) 等价于 max_{a}min_{w,b}L(w,b,a)$$&ensp;&ensp;对$w和b$求偏导可得 $w=\\displaystyle \\sum^m_{i=1} a_iy_ix_i$ $0=\\displaystyle \\sum^m_{i=1} a_iy_i$ &ensp;&ensp;再代入$L(w,b,a)$可得， &ensp;&ensp;最终，约束方程转换为， $$max_a \\displaystyle \\sum^m_{i=1} a_i - {1 \\over 2}\\displaystyle \\sum^m_{i=1} \\displaystyle \\sum^m_{j=1} a_ia_jy_iy_jx_i^Tx_j$$ $$\\displaystyle \\sum^m_{i=1} a_iy_i = 0$$ $$a_i \\geq 0$$ &ensp;&ensp;SVM思想的最终目标就是解出a，求得w，b得到超平面模型 $$f(x) = w^Tx+b = \\displaystyle \\sum^m_{i=1} a_iy_ix_i^Tx+b$$ &ensp;&ensp;根据模型方程和KKT条件可以发现最终模型只与支持向量有关， &ensp;&ensp;1)若$a_i=0$，样本不出现在$f(x)$表达式中，不影响。 &ensp;&ensp;2)若$a_i&gt;0$，则有y_if(x_i) = 1，样本点位于最大间隔边界上，是一个支持向量。 &ensp;&ensp;对于优化方程的求解，我们在后面的SMO算法进行求解。 4、软间隔与正则化（近似线性可分）&ensp;&ensp;现实情况很难找到一个超平面将训练样本完全分开，即使找到也不确定是否是过拟合。 &ensp;&ensp;因此我们允许做到近似线性可分，引入软间隔的概念，允许支持向量机在少量部分样本出错，即不满足约束 $$y_i(w^Tx+b)-1 \\geq 0$$ &ensp;&ensp;优化目标可写成 $$min_{w,b}{1 \\over 2} ||w||^2+C\\displaystyle \\sum^m_{i=1} max(0,1-y_i(w^Tx_i+b))$$ &ensp;&ensp;C>0是惩罚系数，可理解为一般ML问题的正则化参数，C越大，对误分类惩罚越大，当C无穷大时，迫使所有样本分类正确，即硬间隔SVM问题。C越小，对误分类惩罚越小。 &ensp;&ensp;引入松弛变量$\\xi_i$ , 优化方程为， $min_{w,b}{1 \\over 2} ||w||^2+C\\displaystyle \\sum^m_{i=1} \\xi_i$ $s.t. y_i(w^Tx+b) \\geq 1-\\xi_i$ $\\xi_i \\geq 0$ &ensp;&ensp;即软间隔支持向量机。","link":"/2019/09/10/SMO/"},{"title":"Hello, Nanjing University","text":"2019.09.01: I am so glad to come to Nanjing University and start my new study.","link":"/2019/09/06/Hello-Nanjing-University/"}],"tags":[{"name":"Research Life","slug":"Research-Life","link":"/tags/Research-Life/"},{"name":"AI","slug":"AI","link":"/tags/AI/"},{"name":"Machine Learning","slug":"Machine-Learning","link":"/tags/Machine-Learning/"},{"name":"SVM","slug":"SVM","link":"/tags/SVM/"}],"categories":[{"name":"Research Life","slug":"Research-Life","link":"/categories/Research-Life/"},{"name":"Machine Learning","slug":"Machine-Learning","link":"/categories/Machine-Learning/"}]}